name: CI/CD Pipeline for LLaMA Server with KubeRay

on:
  push:
    branches: [ main, master ]
    tags: [ 'v*' ]
  pull_request:
    branches: [ main, master ]
  workflow_dispatch:

env:
  # Configure for Docker Hub
  REGISTRY: docker.io
  IMAGE_NAME: ${{ secrets.DOCKER_HUB_USERNAME }}/llama-server
  RAY_IMAGE_NAME: ${{ secrets.DOCKER_HUB_USERNAME }}/llama-ray

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
      id-token: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Prepare directory structure
        run: |
          mkdir -p metrics configs ray_serve

          # Create metrics exporter script
          cat > metrics/exporter.py << 'EOF'
          #!/usr/bin/env python3
          import time
          import requests
          import json
          import psutil
          import os
          import logging
          from prometheus_client import start_http_server, Gauge, Counter, Histogram, Info
          from prometheus_client.core import CollectorRegistry

          # Set up logging
          logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
          logger = logging.getLogger(__name__)

          # Create registry
          registry = CollectorRegistry()

          # Define metrics
          llama_info = Info('llama_server_info', 'LLaMA server information', registry=registry)
          llama_up = Gauge('llama_server_up', 'LLaMA server status', registry=registry)
          llama_requests_total = Counter('llama_requests_total', 'Total requests processed', registry=registry)
          llama_active_slots = Gauge('llama_active_slots', 'Active processing slots', registry=registry)
          llama_queue_size = Gauge('llama_queue_size', 'Current queue size', registry=registry)
          llama_cpu_usage = Gauge('llama_cpu_usage_percent', 'CPU usage percentage', registry=registry)
          llama_memory_usage = Gauge('llama_memory_usage_bytes', 'Memory usage in bytes', registry=registry)
          llama_model_loaded = Gauge('llama_model_loaded', 'Whether model is loaded', registry=registry)

          class LlamaMetrics:
              def __init__(self, llama_url='http://localhost:8084'):
                  self.llama_url = llama_url
                  self.process = None
                  self.request_count = 0
                  logger.info(f"Initializing metrics collector for {llama_url}")

              def find_llama_process(self):
                  """Find the llama-server process"""
                  try:
                      for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
                          try:
                              cmdline = proc.info.get('cmdline', [])
                              if cmdline and any('llama-server' in str(cmd) for cmd in cmdline):
                                  self.process = psutil.Process(proc.info['pid'])
                                  logger.info(f"Found llama-server process: PID {proc.info['pid']}")
                                  return
                          except (psutil.NoSuchProcess, psutil.AccessDenied, TypeError):
                              continue
                  except Exception as e:
                      logger.error(f"Error finding llama process: {e}")

              def collect_system_metrics(self):
                  """Collect system metrics for the llama process"""
                  if not self.process:
                      self.find_llama_process()

                  if self.process:
                      try:
                          if self.process.is_running():
                              # CPU usage
                              cpu_percent = self.process.cpu_percent()
                              llama_cpu_usage.set(cpu_percent)

                              # Memory usage
                              memory_info = self.process.memory_info()
                              llama_memory_usage.set(memory_info.rss)

                              logger.debug(f"System metrics - CPU: {cpu_percent}%, Memory: {memory_info.rss} bytes")
                          else:
                              self.process = None

                      except (psutil.NoSuchProcess, psutil.AccessDenied) as e:
                          logger.warning(f"Lost access to process: {e}")
                          self.process = None

              def collect_server_metrics(self):
                  """Collect metrics from llama server endpoints"""
                  server_up = False

                  # Test basic connectivity
                  try:
                      response = requests.get(f"{self.llama_url}/health", timeout=3)
                      if response.status_code == 200:
                          server_up = True
                          try:
                              health_data = response.json()
                              logger.debug(f"Health data: {health_data}")

                              # Check if model is loaded
                              if 'status' in health_data:
                                  if health_data['status'] == 'ok':
                                      llama_model_loaded.set(1)
                                  else:
                                      llama_model_loaded.set(0)

                              # Extract slot information if available
                              if 'slots_idle' in health_data and 'slots_processing' in health_data:
                                  llama_active_slots.set(health_data.get('slots_processing', 0))

                          except (json.JSONDecodeError, KeyError) as e:
                              logger.warning(f"Could not parse health response: {e}")

                      else:
                          logger.warning(f"Health check returned status {response.status_code}")

                  except requests.RequestException as e:
                      logger.warning(f"Health check failed: {e}")

                  # Set server status
                  llama_up.set(1 if server_up else 0)

                  # Try slots endpoint for more detailed info
                  if server_up:
                      try:
                          response = requests.get(f"{self.llama_url}/slots", timeout=3)
                          if response.status_code == 200:
                              slots_data = response.json()
                              if isinstance(slots_data, list):
                                  processing_slots = sum(1 for s in slots_data if s.get('is_processing', False))
                                  llama_active_slots.set(processing_slots)
                                  logger.debug(f"Slots: {processing_slots} processing out of {len(slots_data)} total")

                      except requests.RequestException as e:
                          logger.debug(f"Slots endpoint not available: {e}")

                      # FIXED: Properly increment the counter instead of direct assignment
                      llama_requests_total.inc()

              def run(self):
                  """Main metrics collection loop"""
                  logger.info("Starting LLaMA metrics exporter on port 9090")
                  try:
                      start_http_server(9090, registry=registry)
                      logger.info("Metrics HTTP server started successfully")
                  except Exception as e:
                      logger.error(f"Failed to start metrics server: {e}")
                      return

                  # Set server info
                  llama_info.info({
                      'version': '1.0',
                      'model_path': '/models/model.gguf',
                      'server_url': self.llama_url
                  })

                  logger.info("Starting metrics collection loop")

                  while True:
                      try:
                          self.collect_system_metrics()
                          self.collect_server_metrics()
                          logger.debug("Metrics collection cycle completed")
                      except Exception as e:
                          logger.error(f"Error in metrics collection: {e}")

                      time.sleep(15)

          if __name__ == '__main__':
              try:
                  metrics = LlamaMetrics()
                  metrics.run()
              except KeyboardInterrupt:
                  logger.info("Metrics exporter stopped by user")
              except Exception as e:
                  logger.error(f"Fatal error: {e}")
                  raise
          EOF

          # Create supervisor config
          cat > configs/supervisord.conf << 'EOF'
          [supervisord]
          nodaemon=true
          user=root
          pidfile=/tmp/supervisord.pid
          
          [program:llama-server]
          command=/usr/local/bin/llama-server -m /models/model.gguf -c 2048 --host 0.0.0.0 --port 8084 --slots --verbose
          directory=/models
          user=llama
          autostart=true
          autorestart=true
          stdout_logfile=/dev/stdout
          stdout_logfile_maxbytes=0
          stderr_logfile=/dev/stderr
          stderr_logfile_maxbytes=0
          priority=1
          startsecs=10
          
          [program:metrics-exporter]
          command=python3 /app/metrics/exporter.py
          directory=/app/metrics
          user=llama
          autostart=true
          autorestart=true
          stdout_logfile=/dev/stdout
          stdout_logfile_maxbytes=0
          stderr_logfile=/dev/stderr
          stderr_logfile_maxbytes=0
          priority=2
          startsecs=15
          EOF

          # Create Ray Serve script
          cat > ray_serve/llama_serve.py << 'EOF'
          import os
          import subprocess
          import time
          import threading
          from typing import Dict, Any

          import ray
          from ray import serve
          from ray.serve.drivers import DAGDriver
          from ray.serve.deployment_graph import InputNode
          import requests

          # Start the LLaMA server as a subprocess
          def start_llama_server():
              cmd = [
                  "/usr/local/bin/llama-server",
                  "-m", "/models/model.gguf",
                  "-c", "2048",
                  "--host", "0.0.0.0",
                  "--port", "8084",
                  "--slots",
                  "--verbose"
              ]
              process = subprocess.Popen(cmd)
              return process

          # Start metrics exporter
          def start_metrics_exporter():
              cmd = ["python3", "/app/metrics/exporter.py"]
              process = subprocess.Popen(cmd)
              return process

          @serve.deployment(
              num_replicas=1,
              ray_actor_options={"num_cpus": 1, "num_gpus": 0}
          )
          class LlamaDeployment:
              def __init__(self):
                  self.llama_process = start_llama_server()
                  self.metrics_process = start_metrics_exporter()
                  # Wait for the server to start
                  self._wait_for_server()
                  
              def _wait_for_server(self, max_retries=30):
                  for i in range(max_retries):
                      try:
                          response = requests.get("http://localhost:8084/health", timeout=2)
                          if response.status_code == 200:
                              print("LLaMA server is ready")
                              return
                      except:
                          pass
                      time.sleep(2)
                      print(f"Waiting for LLaMA server... ({i+1}/{max_retries})")
                  print("LLaMA server failed to start")
              
              async def __call__(self, request: Dict[str, Any]) -> Dict[str, Any]:
                  # Forward the request to the LLaMA server
                  try:
                      response = requests.post(
                          "http://localhost:8084/completion",
                          json=request,
                          timeout=60
                      )
                      return response.json()
                  except Exception as e:
                      return {"error": str(e)}
              
              def __del__(self):
                  # Clean up processes when the deployment is terminated
                  if hasattr(self, 'llama_process'):
                      self.llama_process.terminate()
                  if hasattr(self, 'metrics_process'):
                      self.metrics_process.terminate()

          # Create the service
          with InputNode() as input_node:
              deployment = LlamaDeployment.bind()

          app = DAGDriver.bind(deployment, input_node)
          EOF

          # Make scripts executable
          chmod +x metrics/exporter.py

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      # Docker Hub login using secrets
      - name: Log into Docker Hub
        if: github.event_name != 'pull_request'
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ secrets.DOCKER_HUB_USERNAME }}
          password: ${{ secrets.DOCKER_HUB_TOKEN }}

      # Extract metadata for the Docker image
      - name: Extract Docker metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=semver,pattern={{version}}
            type=semver,pattern={{major}}.{{minor}}
            type=ref,event=branch
            type=sha,prefix=sha-,format=short

      # This replaces your "Build standard LLaMA server image locally" step
      - name: Build standard LLaMA server image locally
        run: |
          # Always build locally for KIND
          echo "Building LLaMA server image locally for KIND..."
          docker build -t llama-server:latest .
          # Only push to registry if not a PR
          if [[ "${{ github.event_name }}" != "pull_request" ]]; then
            docker tag llama-server:latest ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ steps.meta.outputs.version || 'latest' }}
            
            # Try pushing with retry and continue on failure
            MAX_RETRIES=3
            RETRY_COUNT=0
            SLEEP_TIME=10
            
            until [ $RETRY_COUNT -ge $MAX_RETRIES ]
            do
            echo "Attempt $((RETRY_COUNT+1)) to push image to Docker Hub..."
            
            if docker push ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ steps.meta.outputs.version || 'latest' }}; then
                echo "Successfully pushed image to Docker Hub"
                break
            else
                RETRY_COUNT=$((RETRY_COUNT+1))
                
                if [ $RETRY_COUNT -ge $MAX_RETRIES ]; then
                echo "Failed to push to Docker Hub after $MAX_RETRIES attempts, continuing workflow..."
                break
                fi
                
                SLEEP_TIME=$((SLEEP_TIME * 2))
                echo "Push failed. Retrying in $SLEEP_TIME seconds..."
                sleep $SLEEP_TIME
            fi
            done
            fi
        
            echo "IMAGE_TAG=latest" >> $GITHUB_ENV

      - name: Build Ray image with essential features
        run: |
          cat > Dockerfile.ray << 'EOF'
          FROM python:3.9-slim
          # System dependencies for Ray
          RUN apt-get update && apt-get install -y --no-install-recommends \
              build-essential \
              curl \
              psmisc \
              wget \
              vim \
              git \
              rsync \
              netcat \
              iproute2 \
              && rm -rf /var/lib/apt/lists/*

          # Install Ray with all necessary components
          RUN pip install --no-cache-dir \
              "ray[serve,tune,train,data,air]==2.9.0" \
              prometheus-client \
              requests \
              psutil \
              numpy \
              pandas \
              scikit-learn \
              torch \
              fastapi \
              uvicorn

          # Create directories for LLaMA and Ray
          RUN mkdir -p /models /app/metrics /app/ray_serve /tmp/ray && \
              chmod -R 777 /models /app /tmp/ray

          # Configure Ray system parameters
          RUN echo "* soft nofile 65536" >> /etc/security/limits.conf && \
              echo "* hard nofile 65536" >> /etc/security/limits.conf && \
              echo "fs.file-max=65536" >> /etc/sysctl.conf

          # Copy application files
          COPY metrics/exporter.py /app/metrics/exporter.py
          COPY ray_serve/llama_serve.py /app/ray_serve/llama_serve.py
          RUN chmod +x /app/metrics/exporter.py

          # Set working directory
          WORKDIR /app/ray_serve

          # Create entrypoint script to properly start Ray
          RUN cat > /app/ray_serve/start.sh << 'ENTRYPOINT'
          #!/bin/bash
          # Start Ray head node
          ray start --head \
              --port=6379 \
              --dashboard-host=0.0.0.0 \
              --dashboard-port=8265 \
              --num-cpus="${RAY_NUM_CPUS:-1}" \
              --block &
          
          # Wait for Ray to start up
          sleep 5
          
          # Start Ray Serve application
          python -m ray.serve run llama_serve:app
          
          # Keep container running
          tail -f /dev/null
          ENTRYPOINT
          RUN chmod +x /app/ray_serve/start.sh

          # Environment variables
          ENV RAY_NUM_CPUS=1
          ENV RAY_OBJECT_STORE_MEMORY=1000000000
          ENV RAY_memory_monitor_refresh_ms=0
          ENV RAY_dashboard_agent_listen_port=52365

          # Expose ports
          EXPOSE 6379 8265 8000 10001 52365

          # Default command
          CMD ["./start.sh"]
          EOF
          
    # Build Ray image using the existing ray_serve/llama_serve.py file
      - name: Build Ray image locally
        run: |
          echo "Building Ray image locally..."
          docker build -t llama-ray:latest -f Dockerfile.ray.tmp .

          # Tag for registry reference
          docker tag llama-ray:latest ${{ env.REGISTRY }}/${{ env.RAY_IMAGE_NAME }}:local-ray

          # Only push to registry if not a PR
          if [[ "${{ github.event_name }}" != "pull_request" ]]; then
            # Try pushing with retry logic
            MAX_RETRIES=3
            RETRY_COUNT=0
            SLEEP_TIME=10

            until [ $RETRY_COUNT -ge $MAX_RETRIES ]
            do
              if docker push ${{ env.REGISTRY }}/${{ env.RAY_IMAGE_NAME }}:${{ steps.meta.outputs.version || 'latest' }}-ray; then
                echo "Successfully pushed Ray image to Docker Hub"
                break
              else
                RETRY_COUNT=$((RETRY_COUNT+1))
                if [ $RETRY_COUNT -ge $MAX_RETRIES ]; then
                  echo "Failed to push after $MAX_RETRIES attempts, continuing..."
                  break
                fi
                SLEEP_TIME=$((SLEEP_TIME * 2))
                echo "Push failed. Retrying in $SLEEP_TIME seconds..."
                sleep $SLEEP_TIME
              fi
            done
          fi

          echo "RAY_TAG=latest" >> $GITHUB_ENV
          
        # Rest of your build process...
      - name: Install kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.28.0'

      - name: Install Kind
        uses: helm/kind-action@v1.8.0
        with:
          install_only: true

      - name: Create Kind cluster
        run: |
          cat > kind-config.yaml << EOF
          kind: Cluster
          apiVersion: kind.x-k8s.io/v1alpha4
          nodes:
          - role: control-plane
            extraPortMappings:
            - containerPort: 30080
              hostPort: 30080
              protocol: TCP
            - containerPort: 30090
              hostPort: 30090
              protocol: TCP
            - containerPort: 30300
              hostPort: 30300
              protocol: TCP
            - containerPort: 30900
              hostPort: 30900
              protocol: TCP
            - containerPort: 30085
              hostPort: 30085
              protocol: TCP
            - containerPort: 8265
              hostPort: 8265
              protocol: TCP
          EOF

          kind create cluster --name llama-cluster --config=kind-config.yaml
          
          # Create default StorageClass
          kubectl apply -f - <<EOF
          apiVersion: storage.k8s.io/v1
          kind: StorageClass
          metadata:
            name: standard
            annotations:
              storageclass.kubernetes.io/is-default-class: "true"
          provisioner: rancher.io/local-path
          volumeBindingMode: WaitForFirstConsumer
          EOF
          
          kubectl get nodes
          kubectl get storageclass

      # Load Docker images to KIND - ADD THIS UPDATED VERSION HERE 
      - name: Load Docker images to KIND
        run: |
          echo "Available Docker images:"
          docker images
          
          echo "Loading images into KIND cluster..."
          kind load docker-image llama-server:latest --name llama-cluster
          kind load docker-image llama-ray:latest --name llama-cluster
          
          # Verify images are loaded
          echo "Images in KIND cluster:"
          docker exec llama-cluster-control-plane crictl images

      - name: Install Helm
        uses: azure/setup-helm@v3
        with:
          version: 'v3.12.3'

      # Add Helm repos
      - name: Add Helm repositories
        run: |
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo add kuberay https://ray-project.github.io/kuberay-helm/
          helm repo update

      # Deploy monitoring stack
      - name: Deploy monitoring
        run: |
          # Create monitoring namespace
          kubectl create namespace monitoring --dry-run=client -o yaml | kubectl apply -f -
          
          # Create simplified values for Prometheus
          cat > monitoring/prometheus-values-ci.yaml << EOF
          alertmanager:
            enabled: false
            
          kubeStateMetrics:
            enabled: true
            
          nodeExporter:
            enabled: true
            
          prometheusOperator:
            admissionWebhooks:
              enabled: false
            resources:
              limits:
                cpu: 100m
                memory: 128Mi
              requests:
                cpu: 50m
                memory: 64Mi
                
          prometheus:
            enabled: true
            service:
              type: NodePort
              nodePort: 30900
            prometheusSpec:
              serviceMonitorSelectorNilUsesHelmValues: false
              serviceMonitorSelector: {}
              serviceMonitorNamespaceSelector: {}
              podMonitorSelectorNilUsesHelmValues: false
              podMonitorSelector: {}
              podMonitorNamespaceSelector: {}
              retention: 1h
              resources:
                limits:
                  cpu: 250m
                  memory: 512Mi
                requests:
                  cpu: 100m
                  memory: 256Mi
                  
          grafana:
            enabled: true
            adminPassword: "admin"
            service:
              type: NodePort
              nodePort: 30300
            resources:
              limits:
                cpu: 100m
                memory: 128Mi
              requests:
                cpu: 50m
                memory: 64Mi
            dashboards:
              default:
                ray-dashboard:
                  json: >
                    {
                      "annotations": {
                        "list": [
                          {
                            "builtIn": 1,
                            "datasource": "-- Grafana --",
                            "enable": true,
                            "hide": true,
                            "iconColor": "rgba(0, 211, 255, 1)",
                            "name": "Annotations & Alerts",
                            "type": "dashboard"
                          }
                        ]
                      },
                      "editable": true,
                      "gnetId": null,
                      "graphTooltip": 0,
                      "id": 1,
                      "links": [],
                      "panels": [
                        {
                          "datasource": null,
                          "fieldConfig": {
                            "defaults": {
                              "color": {
                                "mode": "thresholds"
                              },
                              "mappings": [],
                              "thresholds": {
                                "mode": "absolute",
                                "steps": [
                                  {
                                    "color": "red",
                                    "value": null
                                  },
                                  {
                                    "color": "green",
                                    "value": 1
                                  }
                                ]
                              }
                            },
                            "overrides": []
                          },
                          "gridPos": {
                            "h": 6,
                            "w": 6,
                            "x": 0,
                            "y": 0
                          },
                          "id": 2,
                          "options": {
                            "orientation": "auto",
                            "reduceOptions": {
                              "calcs": [
                                "lastNotNull"
                              ],
                              "fields": "",
                              "values": false
                            },
                            "showThresholdLabels": false,
                            "showThresholdMarkers": true
                          },
                          "pluginVersion": "7.5.7",
                          "targets": [
                            {
                              "exemplar": true,
                              "expr": "up{job=\"ray\"}",
                              "interval": "",
                              "legendFormat": "",
                              "refId": "A"
                            }
                          ],
                          "title": "Ray Cluster Status",
                          "type": "gauge"
                        },
                        {
                          "datasource": null,
                          "fieldConfig": {
                            "defaults": {
                              "color": {
                                "mode": "palette-classic"
                              },
                              "custom": {
                                "axisLabel": "",
                                "axisPlacement": "auto",
                                "barAlignment": 0,
                                "drawStyle": "line",
                                "fillOpacity": 10,
                                "gradientMode": "none",
                                "hideFrom": {
                                  "legend": false,
                                  "tooltip": false,
                                  "viz": false
                                },
                                "lineInterpolation": "linear",
                                "lineWidth": 1,
                                "pointSize": 5,
                                "scaleDistribution": {
                                  "type": "linear"
                                },
                                "showPoints": "never",
                                "spanNulls": true,
                                "stacking": {
                                  "group": "A",
                                  "mode": "none"
                                },
                                "thresholdsStyle": {
                                  "mode": "off"
                                }
                              },
                              "mappings": [],
                              "thresholds": {
                                "mode": "absolute",
                                "steps": [
                                  {
                                    "color": "green",
                                    "value": null
                                  }
                                ]
                              },
                              "unit": "percentunit"
                            },
                            "overrides": []
                          },
                          "gridPos": {
                            "h": 8,
                            "w": 12,
                            "x": 0,
                            "y": 6
                          },
                          "id": 4,
                          "options": {
                            "legend": {
                              "calcs": [],
                              "displayMode": "list",
                              "placement": "bottom"
                            },
                            "tooltip": {
                              "mode": "single"
                            }
                          },
                          "pluginVersion": "7.5.7",
                          "targets": [
                            {
                              "exemplar": true,
                              "expr": "rate(process_cpu_seconds_total{job=\"ray\"}[1m])",
                              "interval": "",
                              "legendFormat": "{{instance}}",
                              "refId": "A"
                            }
                          ],
                          "title": "CPU Usage",
                          "type": "timeseries"
                        },
                        {
                          "datasource": null,
                          "fieldConfig": {
                            "defaults": {
                              "color": {
                                "mode": "palette-classic"
                              },
                              "custom": {
                                "axisLabel": "",
                                "axisPlacement": "auto",
                                "barAlignment": 0,
                                "drawStyle": "line",
                                "fillOpacity": 10,
                                "gradientMode": "none",
                                "hideFrom": {
                                  "legend": false,
                                  "tooltip": false,
                                  "viz": false
                                },
                                "lineInterpolation": "linear",
                                "lineWidth": 1,
                                "pointSize": 5,
                                "scaleDistribution": {
                                  "type": "linear"
                                },
                                "showPoints": "never",
                                "spanNulls": true,
                                "stacking": {
                                  "group": "A",
                                  "mode": "none"
                                },
                                "thresholdsStyle": {
                                  "mode": "off"
                                }
                              },
                              "mappings": [],
                              "thresholds": {
                                "mode": "absolute",
                                "steps": [
                                  {
                                    "color": "green",
                                    "value": null
                                  }
                                ]
                              },
                              "unit": "bytes"
                            },
                            "overrides": []
                          },
                          "gridPos": {
                            "h": 8,
                            "w": 12,
                            "x": 12,
                            "y": 6
                          },
                          "id": 6,
                          "options": {
                            "legend": {
                              "calcs": [],
                              "displayMode": "list",
                              "placement": "bottom"
                            },
                            "tooltip": {
                              "mode": "single"
                            }
                          },
                          "pluginVersion": "7.5.7",
                          "targets": [
                            {
                              "exemplar": true,
                              "expr": "process_resident_memory_bytes{job=\"ray\"}",
                              "interval": "",
                              "legendFormat": "{{instance}}",
                              "refId": "A"
                            }
                          ],
                          "title": "Memory Usage",
                          "type": "timeseries"
                        }
                      ],
                      "refresh": "10s",
                      "schemaVersion": 27,
                      "style": "dark",
                      "tags": [],
                      "templating": {
                        "list": []
                      },
                      "time": {
                        "from": "now-1h",
                        "to": "now"
                      },
                      "timepicker": {},
                      "timezone": "",
                      "title": "Ray Dashboard",
                      "uid": "ray-dashboard",
                      "version": 1
                    }
          EOF
          
          # Install Prometheus Stack
          helm upgrade --install prometheus prometheus-community/kube-prometheus-stack \
            --namespace monitoring \
            --values monitoring/prometheus-values-ci.yaml \
            --timeout 5m \
            --wait || echo "Prometheus installation may not be complete, continuing"
          
          echo "Monitoring services:"
          kubectl -n monitoring get services

      # Install KubeRay Operator
      - name: Install KubeRay Operator
        run: |
          # Create namespace for KubeRay
          kubectl create namespace ray-system --dry-run=client -o yaml | kubectl apply -f -
          
          # Install KubeRay operator
          helm install kuberay-operator kuberay/kuberay-operator \
            --namespace ray-system \
            --wait
          
          echo "Verifying KubeRay operator installation:"
          kubectl -n ray-system get pods
          kubectl get crds | grep ray.io

      - name: Deploy Ray Cluster and Service
        run: |
            # Create llama namespace
            kubectl create namespace llama --dry-run=client -o yaml | kubectl apply -f -
            
            # Create small dummy model for testing
            cat <<EOF | kubectl apply -f -
            apiVersion: v1
            kind: ConfigMap
            metadata:
              name: dummy-model
              namespace: llama
            data:
              model.gguf: |
                DUMMY_MODEL_DATA_FOR_TESTING_ONLY
            EOF
            
            # Create PVC for models
            cat <<EOF | kubectl apply -f -
            apiVersion: v1
            kind: PersistentVolumeClaim
            metadata:
              name: llama-models
              namespace: llama
            spec:
              accessModes:
                - ReadWriteOnce
              resources:
                requests:
                  storage: 100Mi
            EOF
            
            # Wait for PVC to be created (no -Ready condition for PVCs, so just sleep)
            echo "Waiting for PVC to be provisioned..."
            sleep 10
            kubectl -n llama get pvc
            
            # Copy dummy model to PVC
            cat <<EOF | kubectl apply -f -
            apiVersion: batch/v1
            kind: Job
            metadata:
              name: model-setup
              namespace: llama
            spec:
              ttlSecondsAfterFinished: 100
              template:
                spec:
                  containers:
                  - name: model-setup
                    image: busybox
                    command: ["sh", "-c", "cp /dummy/model.gguf /models/model.gguf && echo 'Dummy model copied'"]
                    volumeMounts:
                    - name: dummy-model
                      mountPath: /dummy
                    - name: models
                      mountPath: /models
                  volumes:
                  - name: dummy-model
                    configMap:
                      name: dummy-model
                  - name: models
                    persistentVolumeClaim:
                      claimName: llama-models
                  restartPolicy: Never
            EOF
            
            # Wait for job completion
            echo "Waiting for model setup job to complete..."
            kubectl -n llama wait --for=condition=complete --timeout=60s job/model-setup || true
            
            # Create the Ray cluster - FIXED COMPLETE DEFINITION
            cat <<EOF | kubectl apply -f -
            apiVersion: ray.io/v1
            kind: RayCluster
            metadata:
              name: llama-ray-cluster
              namespace: llama
            spec:
              rayVersion: '2.9.0'
              headGroupSpec:
                rayStartParams:
                  dashboard-host: '0.0.0.0'
                  dashboard-port: '8265'
                  port: '6379'
                  block: 'true'
                template:
                  spec:
                    containers:
                      - name: ray-head
                        image: llama-ray:latest
                        imagePullPolicy: IfNotPresent
                        ports:
                          - containerPort: 6379
                            name: gcs
                          - containerPort: 8265
                            name: dashboard
                          - containerPort: 10001
                            name: client
                        resources:
                          limits:
                            cpu: "2"
                            memory: "4Gi"
                          requests:
                            cpu: "1"
                            memory: "2Gi"
                        volumeMounts:
                          - name: models
                            mountPath: /models
                    volumes:
                      - name: models
                        persistentVolumeClaim:
                          claimName: llama-models
              workerGroupSpecs:
                - groupName: small-workers
                  replicas: 1
                  minReplicas: 0
                  maxReplicas: 3
                  rayStartParams: {}
                  template:
                    spec:
                      containers:
                        - name: ray-worker
                          image: llama-ray:latest
                          imagePullPolicy: IfNotPresent
                          resources:
                            limits:
                              cpu: "1"
                              memory: "2Gi"
                            requests:
                              cpu: "500m"
                              memory: "1Gi"
                          volumeMounts:
                            - name: models
                              mountPath: /models
                      volumes:
                        - name: models
                          persistentVolumeClaim:
                            claimName: llama-models
            EOF
            
            # Create the Ray service
            cat <<EOF | kubectl apply -f -
            apiVersion: ray.io/v1
            kind: RayService
            metadata:
              name: llama-ray-service
              namespace: llama
            spec:
              rayClusterName: llama-ray-cluster
              serveConfigV2:
                importPath: llama_serve:app
                runtimeEnv:
                  workingDir: /app/ray_serve
                  pip: []
                deployments:
                  - name: LlamaDeployment
                    numReplicas: 1
                    userConfig: {}
                    rayActorOptions:
                      numCpus: 1
                      numGpus: 0
              service:
                type: NodePort
                port: 8000
                nodePort: 30085
            EOF
            
            # Check deployment status
            echo "RayCluster status:"
            kubectl -n llama get raycluster
            
            echo "RayService status:"
            kubectl -n llama get rayservice || echo "RayService not yet available"
            
            echo "Pods:"
            kubectl -n llama get pods