
name: CI/CD Pipeline for LLaMA Server

on:
  push:
    branches: [ main, master ]
    tags: [ 'v*' ]
  pull_request:
    branches: [ main, master ]
  workflow_dispatch:

env:
  # Configure for Docker Hub
  REGISTRY: docker.io
  IMAGE_NAME: ${{ secrets.DOCKER_HUB_USERNAME }}/llama-server

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
      id-token: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      # ADD THE NEW STEP HERE - after checkout, before Docker build
      - name: Prepare Docker build context
        run: |
          mkdir -p metrics configs
          
          # Create the exporter script
          cat > metrics/exporter.py << 'EOF'
          #!/usr/bin/env python3
          import time
          import requests
          import json
          import psutil
          import os
          import logging
          from prometheus_client import start_http_server, Gauge, Counter, Histogram, Info
          from prometheus_client.core import CollectorRegistry

          # Set up logging
          logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
          logger = logging.getLogger(__name__)

          # Create registry
          registry = CollectorRegistry()

          # Define metrics
          llama_info = Info('llama_server_info', 'LLaMA server information', registry=registry)
          llama_up = Gauge('llama_server_up', 'LLaMA server status', registry=registry)
          llama_requests_total = Counter('llama_requests_total', 'Total requests processed', registry=registry)
          llama_active_slots = Gauge('llama_active_slots', 'Active processing slots', registry=registry)
          llama_queue_size = Gauge('llama_queue_size', 'Current queue size', registry=registry)
          llama_cpu_usage = Gauge('llama_cpu_usage_percent', 'CPU usage percentage', registry=registry)
          llama_memory_usage = Gauge('llama_memory_usage_bytes', 'Memory usage in bytes', registry=registry)
          llama_model_loaded = Gauge('llama_model_loaded', 'Whether model is loaded', registry=registry)

          class LlamaMetrics:
              def __init__(self, llama_url='http://localhost:8084'):
                  self.llama_url = llama_url
                  self.process = None
                  self.request_count = 0
                  logger.info(f"Initializing metrics collector for {llama_url}")

              def find_llama_process(self):
                  """Find the llama-server process"""
                  try:
                      for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
                          try:
                              cmdline = proc.info.get('cmdline', [])
                              if cmdline and any('llama-server' in str(cmd) for cmd in cmdline):
                                  self.process = psutil.Process(proc.info['pid'])
                                  logger.info(f"Found llama-server process: PID {proc.info['pid']}")
                                  return
                          except (psutil.NoSuchProcess, psutil.AccessDenied, TypeError):
                              continue
                  except Exception as e:
                      logger.error(f"Error finding llama process: {e}")

              def collect_system_metrics(self):
                  """Collect system metrics for the llama process"""
                  if not self.process:
                      self.find_llama_process()

                  if self.process:
                      try:
                          if self.process.is_running():
                              # CPU usage
                              cpu_percent = self.process.cpu_percent()
                              llama_cpu_usage.set(cpu_percent)

                              # Memory usage
                              memory_info = self.process.memory_info()
                              llama_memory_usage.set(memory_info.rss)

                              logger.debug(f"System metrics - CPU: {cpu_percent}%, Memory: {memory_info.rss} bytes")
                          else:
                              self.process = None

                      except (psutil.NoSuchProcess, psutil.AccessDenied) as e:
                          logger.warning(f"Lost access to process: {e}")
                          self.process = None

              def collect_server_metrics(self):
                  """Collect metrics from llama server endpoints"""
                  server_up = False

                  # Test basic connectivity
                  try:
                      response = requests.get(f"{self.llama_url}/health", timeout=3)
                      if response.status_code == 200:
                          server_up = True
                          try:
                              health_data = response.json()
                              logger.debug(f"Health data: {health_data}")

                              # Check if model is loaded
                              if 'status' in health_data:
                                  if health_data['status'] == 'ok':
                                      llama_model_loaded.set(1)
                                  else:
                                      llama_model_loaded.set(0)

                              # Extract slot information if available
                              if 'slots_idle' in health_data and 'slots_processing' in health_data:
                                  llama_active_slots.set(health_data.get('slots_processing', 0))

                          except (json.JSONDecodeError, KeyError) as e:
                              logger.warning(f"Could not parse health response: {e}")

                      else:
                          logger.warning(f"Health check returned status {response.status_code}")

                  except requests.RequestException as e:
                      logger.warning(f"Health check failed: {e}")

                  # Set server status
                  llama_up.set(1 if server_up else 0)

                  # Try slots endpoint for more detailed info
                  if server_up:
                      try:
                          response = requests.get(f"{self.llama_url}/slots", timeout=3)
                          if response.status_code == 200:
                              slots_data = response.json()
                              if isinstance(slots_data, list):
                                  processing_slots = sum(1 for s in slots_data if s.get('is_processing', False))
                                  llama_active_slots.set(processing_slots)
                                  logger.debug(f"Slots: {processing_slots} processing out of {len(slots_data)} total")

                      except requests.RequestException as e:
                          logger.debug(f"Slots endpoint not available: {e}")

                      # FIXED: Properly increment the counter instead of direct assignment
                      llama_requests_total.inc()

              def run(self):
                  """Main metrics collection loop"""
                  logger.info("Starting LLaMA metrics exporter on port 9090")
                  try:
                      start_http_server(9090, registry=registry)
                      logger.info("Metrics HTTP server started successfully")
                  except Exception as e:
                      logger.error(f"Failed to start metrics server: {e}")
                      return

                  # Set server info
                  llama_info.info({
                      'version': '1.0',
                      'model_path': '/models/model.gguf',
                      'server_url': self.llama_url
                  })

                  logger.info("Starting metrics collection loop")

                  while True:
                      try:
                          self.collect_system_metrics()
                          self.collect_server_metrics()
                          logger.debug("Metrics collection cycle completed")
                      except Exception as e:
                          logger.error(f"Error in metrics collection: {e}")

                      time.sleep(15)

          if __name__ == '__main__':
              try:
                  metrics = LlamaMetrics()
                  metrics.run()
              except KeyboardInterrupt:
                  logger.info("Metrics exporter stopped by user")
              except Exception as e:
                  logger.error(f"Fatal error: {e}")
                  raise
          EOF
          
          # Create the supervisor config
          cat > configs/supervisord.conf << 'EOF'
          [supervisord]
          nodaemon=true
          user=root
          pidfile=/tmp/supervisord.pid
          
          [program:llama-server]
          command=/usr/local/bin/llama-server -m /models/model.gguf -c 2048 --host 0.0.0.0 --port 8084 --slots --verbose
          directory=/models
          user=llama
          autostart=true
          autorestart=true
          stdout_logfile=/dev/stdout
          stdout_logfile_maxbytes=0
          stderr_logfile=/dev/stderr
          stderr_logfile_maxbytes=0
          priority=1
          startsecs=10
          
          [program:metrics-exporter]
          command=python3 /app/metrics/exporter.py
          directory=/app/metrics
          user=llama
          autostart=true
          autorestart=true
          stdout_logfile=/dev/stdout
          stdout_logfile_maxbytes=0
          stderr_logfile=/dev/stderr
          stderr_logfile_maxbytes=0
          priority=2
          startsecs=15
          EOF
          
          # Make exporter script executable
          chmod +x metrics/exporter.py
          
          # List files to verify
          echo "Created files:"
          find metrics configs -type f | sort

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      # Docker Hub login using secrets
      - name: Log into Docker Hub
        if: github.event_name != 'pull_request'
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ secrets.DOCKER_HUB_USERNAME }}
          password: ${{ secrets.DOCKER_HUB_TOKEN }}

      # Extract metadata for the Docker image
      - name: Extract Docker metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=semver,pattern={{version}}
            type=semver,pattern={{major}}.{{minor}}
            type=ref,event=branch
            type=sha,prefix=sha-,format=short

      # Build and push Docker image
      - name: Build and push Docker image
        id: build-and-push
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile
          push: ${{ github.event_name != 'pull_request' }}
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          no-cache: true  # Disable cache to avoid previous issues

      # Rest of your workflow...
      # ...