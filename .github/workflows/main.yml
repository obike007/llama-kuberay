name: CI/CD Pipeline for LLaMA Server with KubeRay

on:
  push:
    branches: [ main, master ]
    tags: [ 'v*' ]
  pull_request:
    branches: [ main, master ]
  workflow_dispatch:

env:
  # Configure for Docker Hub
  REGISTRY: docker.io
  IMAGE_NAME: ${{ secrets.DOCKER_HUB_USERNAME }}/llama-server
  RAY_IMAGE_NAME: ${{ secrets.DOCKER_HUB_USERNAME }}/llama-ray

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
      id-token: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Prepare directory structure
        run: |
          mkdir -p metrics configs ray_serve

          # Create metrics exporter script
          cat > metrics/exporter.py << 'EOF'
          #!/usr/bin/env python3
          import time
          import requests
          import json
          import psutil
          import os
          import logging
          from prometheus_client import start_http_server, Gauge, Counter, Histogram, Info
          from prometheus_client.core import CollectorRegistry

          # Set up logging
          logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
          logger = logging.getLogger(__name__)

          # Create registry
          registry = CollectorRegistry()

          # Define metrics
          llama_info = Info('llama_server_info', 'LLaMA server information', registry=registry)
          llama_up = Gauge('llama_server_up', 'LLaMA server status', registry=registry)
          llama_requests_total = Counter('llama_requests_total', 'Total requests processed', registry=registry)
          llama_active_slots = Gauge('llama_active_slots', 'Active processing slots', registry=registry)
          llama_queue_size = Gauge('llama_queue_size', 'Current queue size', registry=registry)
          llama_cpu_usage = Gauge('llama_cpu_usage_percent', 'CPU usage percentage', registry=registry)
          llama_memory_usage = Gauge('llama_memory_usage_bytes', 'Memory usage in bytes', registry=registry)
          llama_model_loaded = Gauge('llama_model_loaded', 'Whether model is loaded', registry=registry)

          class LlamaMetrics:
              def __init__(self, llama_url='http://localhost:8084'):
                  self.llama_url = llama_url
                  self.process = None
                  self.request_count = 0
                  logger.info(f"Initializing metrics collector for {llama_url}")

              def find_llama_process(self):
                  """Find the llama-server process"""
                  try:
                      for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
                          try:
                              cmdline = proc.info.get('cmdline', [])
                              if cmdline and any('llama-server' in str(cmd) for cmd in cmdline):
                                  self.process = psutil.Process(proc.info['pid'])
                                  logger.info(f"Found llama-server process: PID {proc.info['pid']}")
                                  return
                          except (psutil.NoSuchProcess, psutil.AccessDenied, TypeError):
                              continue
                  except Exception as e:
                      logger.error(f"Error finding llama process: {e}")

              def collect_system_metrics(self):
                  """Collect system metrics for the llama process"""
                  if not self.process:
                      self.find_llama_process()

                  if self.process:
                      try:
                          if self.process.is_running():
                              # CPU usage
                              cpu_percent = self.process.cpu_percent()
                              llama_cpu_usage.set(cpu_percent)

                              # Memory usage
                              memory_info = self.process.memory_info()
                              llama_memory_usage.set(memory_info.rss)

                              logger.debug(f"System metrics - CPU: {cpu_percent}%, Memory: {memory_info.rss} bytes")
                          else:
                              self.process = None

                      except (psutil.NoSuchProcess, psutil.AccessDenied) as e:
                          logger.warning(f"Lost access to process: {e}")
                          self.process = None

              def collect_server_metrics(self):
                  """Collect metrics from llama server endpoints"""
                  server_up = False

                  # Test basic connectivity
                  try:
                      response = requests.get(f"{self.llama_url}/health", timeout=3)
                      if response.status_code == 200:
                          server_up = True
                          try:
                              health_data = response.json()
                              logger.debug(f"Health data: {health_data}")

                              # Check if model is loaded
                              if 'status' in health_data:
                                  if health_data['status'] == 'ok':
                                      llama_model_loaded.set(1)
                                  else:
                                      llama_model_loaded.set(0)

                              # Extract slot information if available
                              if 'slots_idle' in health_data and 'slots_processing' in health_data:
                                  llama_active_slots.set(health_data.get('slots_processing', 0))

                          except (json.JSONDecodeError, KeyError) as e:
                              logger.warning(f"Could not parse health response: {e}")

                      else:
                          logger.warning(f"Health check returned status {response.status_code}")

                  except requests.RequestException as e:
                      logger.warning(f"Health check failed: {e}")

                  # Set server status
                  llama_up.set(1 if server_up else 0)

                  # Try slots endpoint for more detailed info
                  if server_up:
                      try:
                          response = requests.get(f"{self.llama_url}/slots", timeout=3)
                          if response.status_code == 200:
                              slots_data = response.json()
                              if isinstance(slots_data, list):
                                  processing_slots = sum(1 for s in slots_data if s.get('is_processing', False))
                                  llama_active_slots.set(processing_slots)
                                  logger.debug(f"Slots: {processing_slots} processing out of {len(slots_data)} total")

                      except requests.RequestException as e:
                          logger.debug(f"Slots endpoint not available: {e}")

                      # FIXED: Properly increment the counter instead of direct assignment
                      llama_requests_total.inc()

              def run(self):
                  """Main metrics collection loop"""
                  logger.info("Starting LLaMA metrics exporter on port 9090")
                  try:
                      start_http_server(9090, registry=registry)
                      logger.info("Metrics HTTP server started successfully")
                  except Exception as e:
                      logger.error(f"Failed to start metrics server: {e}")
                      return

                  # Set server info
                  llama_info.info({
                      'version': '1.0',
                      'model_path': '/models/model.gguf',
                      'server_url': self.llama_url
                  })

                  logger.info("Starting metrics collection loop")

                  while True:
                      try:
                          self.collect_system_metrics()
                          self.collect_server_metrics()
                          logger.debug("Metrics collection cycle completed")
                      except Exception as e:
                          logger.error(f"Error in metrics collection: {e}")

                      time.sleep(15)

          if __name__ == '__main__':
              try:
                  metrics = LlamaMetrics()
                  metrics.run()
              except KeyboardInterrupt:
                  logger.info("Metrics exporter stopped by user")
              except Exception as e:
                  logger.error(f"Fatal error: {e}")
                  raise
          EOF

          # Create supervisor config
          cat > configs/supervisord.conf << 'EOF'
          [supervisord]
          nodaemon=true
          user=root
          pidfile=/tmp/supervisord.pid
          
          [program:llama-server]
          command=/usr/local/bin/llama-server -m /models/model.gguf -c 2048 --host 0.0.0.0 --port 8084 --slots --verbose
          directory=/models
          user=llama
          autostart=true
          autorestart=true
          stdout_logfile=/dev/stdout
          stdout_logfile_maxbytes=0
          stderr_logfile=/dev/stderr
          stderr_logfile_maxbytes=0
          priority=1
          startsecs=10
          
          [program:metrics-exporter]
          command=python3 /app/metrics/exporter.py
          directory=/app/metrics
          user=llama
          autostart=true
          autorestart=true
          stdout_logfile=/dev/stdout
          stdout_logfile_maxbytes=0
          stderr_logfile=/dev/stderr
          stderr_logfile_maxbytes=0
          priority=2
          startsecs=15
          EOF

          # Create Ray Serve script
          cat > ray_serve/llama_serve.py << 'EOF'
          import os
          import subprocess
          import time
          import threading
          from typing import Dict, Any

          import ray
          from ray import serve
          from ray.serve.drivers import DAGDriver
          from ray.serve.deployment_graph import InputNode
          import requests

          # Start the LLaMA server as a subprocess
          def start_llama_server():
              cmd = [
                  "/usr/local/bin/llama-server",
                  "-m", "/models/model.gguf",
                  "-c", "2048",
                  "--host", "0.0.0.0",
                  "--port", "8084",
                  "--slots",
                  "--verbose"
              ]
              process = subprocess.Popen(cmd)
              return process

          # Start metrics exporter
          def start_metrics_exporter():
              cmd = ["python3", "/app/metrics/exporter.py"]
              process = subprocess.Popen(cmd)
              return process

          @serve.deployment(
              num_replicas=1,
              ray_actor_options={"num_cpus": 1, "num_gpus": 0}
          )
          class LlamaDeployment:
              def __init__(self):
                  self.llama_process = start_llama_server()
                  self.metrics_process = start_metrics_exporter()
                  # Wait for the server to start
                  self._wait_for_server()
                  
              def _wait_for_server(self, max_retries=30):
                  for i in range(max_retries):
                      try:
                          response = requests.get("http://localhost:8084/health", timeout=2)
                          if response.status_code == 200:
                              print("LLaMA server is ready")
                              return
                      except:
                          pass
                      time.sleep(2)
                      print(f"Waiting for LLaMA server... ({i+1}/{max_retries})")
                  print("LLaMA server failed to start")
              
              async def __call__(self, request: Dict[str, Any]) -> Dict[str, Any]:
                  # Forward the request to the LLaMA server
                  try:
                      response = requests.post(
                          "http://localhost:8084/completion",
                          json=request,
                          timeout=60
                      )
                      return response.json()
                  except Exception as e:
                      return {"error": str(e)}
              
              def __del__(self):
                  # Clean up processes when the deployment is terminated
                  if hasattr(self, 'llama_process'):
                      self.llama_process.terminate()
                  if hasattr(self, 'metrics_process'):
                      self.metrics_process.terminate()

          # Create the service
          with InputNode() as input_node:
              deployment = LlamaDeployment.bind()

          app = DAGDriver.bind(deployment, input_node)
          EOF

          # Make scripts executable
          chmod +x metrics/exporter.py

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      # Docker Hub login using secrets
      - name: Log into Docker Hub
        if: github.event_name != 'pull_request'
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ secrets.DOCKER_HUB_USERNAME }}
          password: ${{ secrets.DOCKER_HUB_TOKEN }}

      # Extract metadata for the Docker image
      - name: Extract Docker metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=semver,pattern={{version}}
            type=semver,pattern={{major}}.{{minor}}
            type=ref,event=branch
            type=sha,prefix=sha-,format=short

      # Build and push Docker image for LLaMA server
      - name: Build and push LLaMA server image
        id: build-and-push
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile
          push: ${{ github.event_name != 'pull_request' }}
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          no-cache: true

      # Build and push Ray integration image
      - name: Build and push Ray integration image
        run: |
          # Set Ray image tag
          RAY_TAG="${{ steps.meta.outputs.version || 'latest' }}-ray"
          echo "RAY_TAG=$RAY_TAG" >> $GITHUB_ENV

          # Create a temporary Dockerfile for Ray image - NO MULTI-STAGE
          cat > Dockerfile.ray.tmp << 'EOF'
          # Simple Ray image without multi-stage complexities
          FROM rayproject/ray:2.9.0

          # Switch to root user forinstallation and setup
          USER root

          # Install Python dependencies for LLaMA and metrics
          RUN pip install prometheus-client requests psutil

          # Create directories for LLaMA
          RUN mkdir -p /models /app/metrics /app/ray_serve && \
              chmod -R 777 /models /app

          # Create lib directory
          RUN mkdir -p /usr/local/lib /usr/local/bin
          
          # We'll download llama.cpp and build the server directly
          RUN apt-get update && apt-get install -y \
              build-essential \
              cmake \
              git \
              libcurl4-openssl-dev \
              pkg-config \
              wget \
              && rm -rf /var/lib/apt/lists/*

          # Build llama.cpp
          WORKDIR /tmp/build
          RUN wget -O llama-cpp.tar.gz https://github.com/ggerganov/llama.cpp/archive/refs/heads/master.tar.gz && \
              tar -xzf llama-cpp.tar.gz && \
              mv llama.cpp-master llama.cpp
              
          WORKDIR /tmp/build/llama.cpp
          RUN cmake -B build \
              -DCMAKE_BUILD_TYPE=Release \
              -DGGML_BLAS=OFF \
              -DGGML_CUBLAS=OFF \
              -DGGML_METAL=OFF \
              -DGGML_HIPBLAS=OFF \
              -DGGML_ACCELERATE=OFF \
              -DLLAMA_BUILD_TESTS=OFF \
              -DLLAMA_BUILD_EXAMPLES=OFF \
              -DLLAMA_BUILD_SERVER=ON \
              && cmake --build build --config Release --target llama-server -j$(nproc)
              
          # Copy the binary to the expected location
          RUN cp /tmp/build/llama.cpp/build/bin/llama-server /usr/local/bin/llama-server && \
              chmod +x /usr/local/bin/llama-server
              
          # Copy any shared libraries
          RUN if [ -d /tmp/build/llama.cpp/build/lib ]; then \
                cp -r /tmp/build/llama.cpp/build/lib/* /usr/local/lib/ 2>/dev/null || true; \
              fi && \
              ldconfig

          # Copy the metrics exporter script and Ray serve script
          COPY metrics/exporter.py /app/metrics/exporter.py
          RUN chmod +x /app/metrics/exporter.py
          COPY ray_serve/llama_serve.py /app/ray_serve/llama_serve.py
          EOF

          # Clean up build files to reduce image size
          RUN rm -rf /tmp/build

          # Switch back to ray user for running the application
          USER ray

          # Set working directory
          WORKDIR /app/ray_serve

          # Command to run (optional, as Ray images typically have their own entrypoint)
          # CMD ["python", "llama_serve.py"]
          EOF

          
          # Display the Dockerfile for debugging
          echo "=== Generated Dockerfile.ray.tmp ==="
          cat Dockerfile.ray.tmp
          echo "=== End of Dockerfile ==="
          
          if [[ "${{ github.event_name }}" != "pull_request" ]]; then
            # For actual pushes, build and push to Docker Hub
            docker build -t ${{ env.REGISTRY }}/${{ env.RAY_IMAGE_NAME }}:${RAY_TAG} -f Dockerfile.ray.tmp .
            docker push ${{ env.REGISTRY }}/${{ env.RAY_IMAGE_NAME }}:${RAY_TAG}
          else
            # For PRs, just build locally
            docker build -t ${{ env.REGISTRY }}/${{ env.RAY_IMAGE_NAME }}:${RAY_TAG} -f Dockerfile.ray.tmp .
          fi

      # Install kubectl and kind
      - name: Install kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.28.0'

      - name: Install Kind
        uses: helm/kind-action@v1.8.0
        with:
          install_only: true

      - name: Create Kind cluster
        run: |
          cat > kind-config.yaml << EOF
          kind: Cluster
          apiVersion: kind.x-k8s.io/v1alpha4
          nodes:
          - role: control-plane
            extraPortMappings:
            - containerPort: 30080
              hostPort: 30080
              protocol: TCP
            - containerPort: 30090
              hostPort: 30090
              protocol: TCP
            - containerPort: 30300
              hostPort: 30300
              protocol: TCP
            - containerPort: 30900
              hostPort: 30900
              protocol: TCP
            - containerPort: 30085
              hostPort: 30085
              protocol: TCP
            - containerPort: 8265
              hostPort: 8265
              protocol: TCP
          EOF

          kind create cluster --name llama-cluster --config=kind-config.yaml
          
          # Create default StorageClass
          kubectl apply -f - <<EOF
          apiVersion: storage.k8s.io/v1
          kind: StorageClass
          metadata:
            name: standard
            annotations:
              storageclass.kubernetes.io/is-default-class: "true"
          provisioner: rancher.io/local-path
          volumeBindingMode: WaitForFirstConsumer
          EOF
          
          kubectl get nodes
          kubectl get storageclass

      - name: Install Helm
        uses: azure/setup-helm@v3
        with:
          version: 'v3.12.3'

      # Add Helm repos
      - name: Add Helm repositories
        run: |
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo add kuberay https://ray-project.github.io/kuberay-helm/
          helm repo update

      # Deploy monitoring stack
      - name: Deploy monitoring
        run: |
          # Create monitoring namespace
          kubectl create namespace monitoring --dry-run=client -o yaml | kubectl apply -f -
          
          # Create simplified values for Prometheus
          cat > monitoring/prometheus-values-ci.yaml << EOF
          alertmanager:
            enabled: false
            
          kubeStateMetrics:
            enabled: true
            
          nodeExporter:
            enabled: true
            
          prometheusOperator:
            admissionWebhooks:
              enabled: false
            resources:
              limits:
                cpu: 100m
                memory: 128Mi
              requests:
                cpu: 50m
                memory: 64Mi
                
          prometheus:
            enabled: true
            service:
              type: NodePort
              nodePort: 30900
            prometheusSpec:
              serviceMonitorSelectorNilUsesHelmValues: false
              serviceMonitorSelector: {}
              serviceMonitorNamespaceSelector: {}
              podMonitorSelectorNilUsesHelmValues: false
              podMonitorSelector: {}
              podMonitorNamespaceSelector: {}
              retention: 1h
              resources:
                limits:
                  cpu: 250m
                  memory: 512Mi
                requests:
                  cpu: 100m
                  memory: 256Mi
                  
          grafana:
            enabled: true
            adminPassword: "admin"
            service:
              type: NodePort
              nodePort: 30300
            resources:
              limits:
                cpu: 100m
                memory: 128Mi
              requests:
                cpu: 50m
                memory: 64Mi
            dashboards:
              default:
                ray-dashboard:
                  json: >
                    {
                      "annotations": {
                        "list": [
                          {
                            "builtIn": 1,
                            "datasource": "-- Grafana --",
                            "enable": true,
                            "hide": true,
                            "iconColor": "rgba(0, 211, 255, 1)",
                            "name": "Annotations & Alerts",
                            "type": "dashboard"
                          }
                        ]
                      },
                      "editable": true,
                      "gnetId": null,
                      "graphTooltip": 0,
                      "id": 1,
                      "links": [],
                      "panels": [
                        {
                          "datasource": null,
                          "fieldConfig": {
                            "defaults": {
                              "color": {
                                "mode": "thresholds"
                              },
                              "mappings": [],
                              "thresholds": {
                                "mode": "absolute",
                                "steps": [
                                  {
                                    "color": "red",
                                    "value": null
                                  },
                                  {
                                    "color": "green",
                                    "value": 1
                                  }
                                ]
                              }
                            },
                            "overrides": []
                          },
                          "gridPos": {
                            "h": 6,
                            "w": 6,
                            "x": 0,
                            "y": 0
                          },
                          "id": 2,
                          "options": {
                            "orientation": "auto",
                            "reduceOptions": {
                              "calcs": [
                                "lastNotNull"
                              ],
                              "fields": "",
                              "values": false
                            },
                            "showThresholdLabels": false,
                            "showThresholdMarkers": true
                          },
                          "pluginVersion": "7.5.7",
                          "targets": [
                            {
                              "exemplar": true,
                              "expr": "up{job=\"ray\"}",
                              "interval": "",
                              "legendFormat": "",
                              "refId": "A"
                            }
                          ],
                          "title": "Ray Cluster Status",
                          "type": "gauge"
                        },
                        {
                          "datasource": null,
                          "fieldConfig": {
                            "defaults": {
                              "color": {
                                "mode": "palette-classic"
                              },
                              "custom": {
                                "axisLabel": "",
                                "axisPlacement": "auto",
                                "barAlignment": 0,
                                "drawStyle": "line",
                                "fillOpacity": 10,
                                "gradientMode": "none",
                                "hideFrom": {
                                  "legend": false,
                                  "tooltip": false,
                                  "viz": false
                                },
                                "lineInterpolation": "linear",
                                "lineWidth": 1,
                                "pointSize": 5,
                                "scaleDistribution": {
                                  "type": "linear"
                                },
                                "showPoints": "never",
                                "spanNulls": true,
                                "stacking": {
                                  "group": "A",
                                  "mode": "none"
                                },
                                "thresholdsStyle": {
                                  "mode": "off"
                                }
                              },
                              "mappings": [],
                              "thresholds": {
                                "mode": "absolute",
                                "steps": [
                                  {
                                    "color": "green",
                                    "value": null
                                  }
                                ]
                              },
                              "unit": "percentunit"
                            },
                            "overrides": []
                          },
                          "gridPos": {
                            "h": 8,
                            "w": 12,
                            "x": 0,
                            "y": 6
                          },
                          "id": 4,
                          "options": {
                            "legend": {
                              "calcs": [],
                              "displayMode": "list",
                              "placement": "bottom"
                            },
                            "tooltip": {
                              "mode": "single"
                            }
                          },
                          "pluginVersion": "7.5.7",
                          "targets": [
                            {
                              "exemplar": true,
                              "expr": "rate(process_cpu_seconds_total{job=\"ray\"}[1m])",
                              "interval": "",
                              "legendFormat": "{{instance}}",
                              "refId": "A"
                            }
                          ],
                          "title": "CPU Usage",
                          "type": "timeseries"
                        },
                        {
                          "datasource": null,
                          "fieldConfig": {
                            "defaults": {
                              "color": {
                                "mode": "palette-classic"
                              },
                              "custom": {
                                "axisLabel": "",
                                "axisPlacement": "auto",
                                "barAlignment": 0,
                                "drawStyle": "line",
                                "fillOpacity": 10,
                                "gradientMode": "none",
                                "hideFrom": {
                                  "legend": false,
                                  "tooltip": false,
                                  "viz": false
                                },
                                "lineInterpolation": "linear",
                                "lineWidth": 1,
                                "pointSize": 5,
                                "scaleDistribution": {
                                  "type": "linear"
                                },
                                "showPoints": "never",
                                "spanNulls": true,
                                "stacking": {
                                  "group": "A",
                                  "mode": "none"
                                },
                                "thresholdsStyle": {
                                  "mode": "off"
                                }
                              },
                              "mappings": [],
                              "thresholds": {
                                "mode": "absolute",
                                "steps": [
                                  {
                                    "color": "green",
                                    "value": null
                                  }
                                ]
                              },
                              "unit": "bytes"
                            },
                            "overrides": []
                          },
                          "gridPos": {
                            "h": 8,
                            "w": 12,
                            "x": 12,
                            "y": 6
                          },
                          "id": 6,
                          "options": {
                            "legend": {
                              "calcs": [],
                              "displayMode": "list",
                              "placement": "bottom"
                            },
                            "tooltip": {
                              "mode": "single"
                            }
                          },
                          "pluginVersion": "7.5.7",
                          "targets": [
                            {
                              "exemplar": true,
                              "expr": "process_resident_memory_bytes{job=\"ray\"}",
                              "interval": "",
                              "legendFormat": "{{instance}}",
                              "refId": "A"
                            }
                          ],
                          "title": "Memory Usage",
                          "type": "timeseries"
                        }
                      ],
                      "refresh": "10s",
                      "schemaVersion": 27,
                      "style": "dark",
                      "tags": [],
                      "templating": {
                        "list": []
                      },
                      "time": {
                        "from": "now-1h",
                        "to": "now"
                      },
                      "timepicker": {},
                      "timezone": "",
                      "title": "Ray Dashboard",
                      "uid": "ray-dashboard",
                      "version": 1
                    }
          EOF
          
          # Install Prometheus Stack
          helm upgrade --install prometheus prometheus-community/kube-prometheus-stack \
            --namespace monitoring \
            --values monitoring/prometheus-values-ci.yaml \
            --timeout 5m \
            --wait || echo "Prometheus installation may not be complete, continuing"
          
          echo "Monitoring services:"
          kubectl -n monitoring get services

      # Load Docker images to KIND
      - name: Load Docker images to KIND
        run: |
          if [[ "${{ github.event_name }}" == "pull_request" ]]; then
            # For PRs, use locally built images
            echo "Using locally built images for pull request..."
            VERSION="pr-${{ github.event.pull_request.number || 'local' }}"
            
            # Build local images if they don't exist
            docker tag ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ steps.meta.outputs.version || 'latest' }} ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${VERSION}
            docker tag ${{ env.REGISTRY }}/${{ env.RAY_IMAGE_NAME }}:${{ env.RAY_TAG }} ${{ env.REGISTRY }}/${{ env.RAY_IMAGE_NAME }}:${VERSION}-ray
          else
            # For non-PRs, use the versioned images we pushed
            VERSION="${{ steps.meta.outputs.version || 'latest' }}"
          fi
          
          echo "Loading images into KIND cluster..."
          kind load docker-image ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${VERSION} --name llama-cluster
          kind load docker-image ${{ env.REGISTRY }}/${{ env.RAY_IMAGE_NAME }}:${{ env.RAY_TAG }} --name llama-cluster
          
          echo "IMAGE_VERSION=${VERSION}" >> $GITHUB_ENV
          echo "Loaded images:"
          docker images | grep llama

      # Install KubeRay Operator
      - name: Install KubeRay Operator
        run: |
          # Create namespace for KubeRay
          kubectl create namespace ray-system --dry-run=client -o yaml | kubectl apply -f -
          
          # Install KubeRay operator
          helm install kuberay-operator kuberay/kuberay-operator \
            --namespace ray-system \
            --wait
          
          echo "Verifying KubeRay operator installation:"
          kubectl -n ray-system get pods
          kubectl get crds | grep ray.io

      # Add this step after installing the KubeRay operator
      - name: Deploy with KubeRay
        run: |
          # Make the script executable
          chmod +x scripts/deploy-kuberay.sh

          # Deploy using the script
          ./scripts/deploy-kuberay.sh ${{ env.REGISTRY }}/${{ env.RAY_IMAGE_NAME }} ${{ env.RAY_TAG }}

          # Wait for deployment
          echo "Waiting for Ray Cluster to be ready..."
          kubectl -n llama wait --for=condition=established --timeout=5m \
            crd/rayclusters.ray.io \
            crd/rayservices.ray.io
          
          kubectl -n llama wait --for=condition=ready --timeout=5m rayservice/llama-ray-service || true
          
          # Show deployment status
          echo "Ray Service status:"
          kubectl -n llama get rayservice
          
          echo "Ray Cluster status:"
          kubectl -n llama get raycluster
          
          echo "Ray Pods:"
          kubectl -n llama get pods -l ray.io/cluster=llama-ray-cluster 
          # Deploy LLaMA with Helm and KubeRay
      - name: Deploy LLaMA with Helm
        run: |
          # Create llama namespace
          kubectl create namespace llama --dry-run=client -o yaml | kubectl apply -f -
          
          # Create small dummy model for testing
          cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: dummy-model
            namespace: llama
          data:
            model.gguf: |
              DUMMY_MODEL_DATA_FOR_TESTING_ONLY
          EOF
          
          # Create PVC for models
          cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: PersistentVolumeClaim
          metadata:
            name: llama-models
            namespace: llama
          spec:
            accessModes:
              - ReadWriteOnce
            resources:
              requests:
                storage: 100Mi
          EOF
          
          # Wait for PVC to be created
          kubectl -n llama wait --for=condition=Ready --timeout=30s pvc/llama-models || true
          
          # Copy dummy model to PVC
          cat <<EOF | kubectl apply -f -
          apiVersion: batch/v1
          kind: Job
          metadata:
            name: model-setup
            namespace: llama
          spec:
            ttlSecondsAfterFinished: 100
            template:
              spec:
                containers:
                - name: model-setup
                  image: busybox
                  command: ["sh", "-c", "cp /dummy/model.gguf /models/model.gguf && echo 'Dummy model copied'"]
                  volumeMounts:
                  - name: dummy-model
                    mountPath: /dummy
                  - name: models
                    mountPath: /models
                volumes:
                - name: dummy-model