# Add this to your existing values.yaml
ray:
  enabled: true
  version: "2.9.0"
  cluster:
    name: "llama-ray-cluster"
  image:
    repository: "llama-ray"  # Will be overridden in CI/CD
    tag: "latest"           # Will be overridden in CI/CD
    pullPolicy: "IfNotPresent"
  headResources:
    limits:
      cpu: "2"
      memory: "4Gi"
    requests:
      cpu: "1"
      memory: "2Gi"
  workerResources:
    limits:
      cpu: "1"
      memory: "2Gi"
    requests:
      cpu: "500m"
      memory: "1Gi"
  workers:
    replicas: 1
    minReplicas: 0
    maxReplicas: 3
  service:
    enabled: true
    apiVersion: "v1"  # Set this to match your KubeRay version: "v1alpha1", "v1-deploymentgraph", or "v1"
    name: "llama-ray-service"
    importPath: "llama_serve:app"
    numReplicas: 1
    cpuPerReplica: 1
    gpuPerReplica: 0
    type: "NodePort"
    port: 8000
    nodePort: 30085
    pip: []
    userConfig: {}

# Ensure your persistence settings are correct
persistence:
  enabled: true
  name: "llama-models"
  size: "100Mi"
  mountPath: "/models"
  storageClass: "standard"